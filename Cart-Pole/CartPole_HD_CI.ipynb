{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Student\\Desktop\\vs_git\\.virtualenvs\\venv\\lib\\site-packages\\ale_py\\roms\\utils.py:90: DeprecationWarning: SelectableGroups dict interface is deprecated. Use select.\n",
      "  for external in metadata.entry_points().get(self.group, []):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Student\\Desktop\\vs_git\\.virtualenvs\\venv\\lib\\site-packages\\stable_baselines\\__init__.py:33: UserWarning: stable-baselines is in maintenance mode, please use [Stable-Baselines3 (SB3)](https://github.com/DLR-RM/stable-baselines3) for an up-to-date version. You can find a [migration guide](https://stable-baselines3.readthedocs.io/en/master/guide/migration.html) in SB3 documentation.\n",
      "  \"stable-baselines is in maintenance mode, please use [Stable-Baselines3 (SB3)](https://github.com/DLR-RM/stable-baselines3) for an up-to-date version. You can find a [migration guide](https://stable-baselines3.readthedocs.io/en/master/guide/migration.html) in SB3 documentation.\"\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import gym\n",
    "import numpy as np\n",
    "from stable_baselines import DQN #type: ignore\n",
    "from copy import deepcopy\n",
    "import math\n",
    "from gym.spaces import Discrete, Dict, Box\n",
    "from gym import spaces\n",
    "from random import seed\n",
    "import random \n",
    "from gym import Env\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "import stable_baselines\n",
    "import sklearn\n",
    "from sklearn import tree , svm \n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB , CategoricalNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix #type: ignore\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from itertools import product\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import KFold , RepeatedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import impute\n",
    "import statistics\n",
    "from scipy import stats\n",
    "from copy import deepcopy\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from math import ceil\n",
    "import copy\n",
    "import sys\n",
    "from sklearn.metrics import jaccard_score\n",
    "import time\n",
    "import multiprocessing\n",
    "from pymoo.algorithms.nsga2 import calc_crowding_distance\n",
    "sys.path.append('lib/')\n",
    "import subprocess\n",
    "import logging\n",
    "from sklearn.utils import shuffle\n",
    "import csv\n",
    "from csv import reader\n",
    "import os\n",
    "from sklearn.metrics import balanced_accuracy_score, precision_score, recall_score\n",
    "\n",
    "\n",
    "class StoreWrapper(gym.Wrapper):\n",
    "  ''''\n",
    "  :param env: (gym.Env) Gym environment that will be wrapped\n",
    "  :param max_steps: (int) Max number of steps per episode\n",
    "  '''\n",
    "  def __init__(self, env):\n",
    "    # Call the parent constructor, so we can access self.env later\n",
    "    super(StoreWrapper, self).__init__(env)\n",
    "    self.max_steps = 200\n",
    "    # Counter of steps per episode\n",
    "    self.current_step = 0\n",
    "    self.mem = []\n",
    "    self.TotalReward = 0.0 \n",
    "    self.env = env\n",
    "    self.first_state = 0\n",
    "    self.first_obs = 0\n",
    "    self.prev_obs = 0 \n",
    "    self.states_list = []\n",
    "    self.info = {}\n",
    "  \n",
    "  def reset(self):\n",
    "    \"\"\"\n",
    "    Reset the environment \n",
    "    \"\"\"\n",
    "    # Reset the counter\n",
    "    self.current_step = 0\n",
    "    obs =self.env.reset()\n",
    "    self.TotalReward = 0.0\n",
    "    self.first_obs = obs\n",
    "    return obs\n",
    "\n",
    "  def step(self, action):\n",
    "    \"\"\"\n",
    "    In this function we store the initial state as well as the memory of the agent\n",
    "    :param action: ([float] or int) Action taken by the agent\n",
    "    :return: (np.ndarray, float, bool, dict) observation, reward, is the episode over?, additional informations\n",
    "    \"\"\"\n",
    "    if self.current_step == 0: #store initial state\n",
    "      self.prev_obs = self.first_obs\n",
    "      self.first_state = deepcopy(self.env)\n",
    "      self.states_list.append(self.first_state)\n",
    "    self.current_step += 1\n",
    "    obs, reward, done, info = self.env.step(action)\n",
    "    self.TotalReward += reward\n",
    "    self.mem.append(tuple((self.prev_obs,action)))\n",
    "    self.prev_obs = obs\n",
    "    if self.current_step >= self.max_steps:\n",
    "      done = True\n",
    "      # Update the info dict to signal that the limit was exceeded\n",
    "    if done:\n",
    "      self.mem.append(tuple(('done',self.TotalReward)))\n",
    "    self.info['mem'] = self.mem\n",
    "    self.info['state'] = self.states_list\n",
    "    # self.mem.append(tuple(obs,action))\n",
    "    return obs, reward, done, info\n",
    "\n",
    "  def set_state(self, state):\n",
    "    \"\"\"\n",
    "    :param state: initial state of the episode\n",
    "    :return: environment is updated and observations is returned\n",
    "    \"\"\"\n",
    "    self.env = deepcopy(state)\n",
    "    obs = np.array(list(self.env.unwrapped.state))\n",
    "    self.current_step = 0\n",
    "    self.TotalReward = 0.0\n",
    "    self.first_obs = obs\n",
    "    return obs\n",
    "\n",
    "def abstract_state(model,state1,d):\n",
    "  if type(state1) == str:\n",
    "    if state1 == 'done':\n",
    "      return 'end'\n",
    "  q_value1 = model.step_model.step([state1])\n",
    "  return( ceil(q_value1[1][0][0]/d), ceil(q_value1[1][0][1]/d))\n",
    "\n",
    "def abstract_state_general(model,state1,d):\n",
    "  if type(state1) == str:\n",
    "    if state1 == 'done':\n",
    "      return 'end'\n",
    "  q_values = model.step_model.step([state1])\n",
    "  return tuple([ceil(q_value/d) for q_value in q_values[1][0]])\n",
    "\n",
    "def Abstract_classes(ep,abstraction_d,model):\n",
    "  d=abstraction_d\n",
    "  abs_states1=[]\n",
    "  for episode in ep:\n",
    "    for state,action in episode:\n",
    "      abs_st = abstract_state(model,state,d)\n",
    "      if abs_st == 'end':\n",
    "        continue\n",
    "      abs_states1.append(abs_st)\n",
    "  unique1=list(set(abs_states1))\n",
    "  uni1 = np.array(unique1)\n",
    "  a=len(abs_states1)\n",
    "  b=len(set(abs_states1))\n",
    "  print(\"abstract states:\",b)\n",
    "  print(\"Concrete states\",a)\n",
    "  print(\"ratio\",b/a)\n",
    "  return unique1,uni1\n",
    "\n",
    "def is_functional_fault(episode,epsilon):\n",
    "  for state, _ in episode:\n",
    "    if type(state) == str:\n",
    "      if state == 'done':\n",
    "        return False\n",
    "    if abs(state[0]) >= (2.4-epsilon):\n",
    "      return True\n",
    "  # no functional fault is obsereved\n",
    "  return False\n",
    "\n",
    "def is_reward_fault(episode,threshold):\n",
    "  last_state = episode[-1]\n",
    "  assert last_state[0] == 'done'\n",
    "  if last_state[1] < threshold:\n",
    "    return True\n",
    "  else:\n",
    "    return False\n",
    "\n",
    "def ML_first_representation_func_based(Abs_d,functional_func,reward_func,model,input_episodes,unique1,epsilon,threshold):\n",
    "  \"\"\"\n",
    "  TO-DO : fix epsilon and threshold\n",
    "  \"\"\"\n",
    "  d = Abs_d\n",
    "  data1_x_b=[]\n",
    "  data1_y_b= [] \n",
    "  data1_y_f_b = []\n",
    "  for i, episode in enumerate(input_episodes):\n",
    "    record = np.zeros(len(unique1))\n",
    "    temp_flag = False\n",
    "    for state, action in episode:\n",
    "      ab = abstract_state(model,state,d)\n",
    "      if ab == 'end':\n",
    "        assert not temp_flag, f'Episode data problem, two terminations in one episode. Episode number{i}'\n",
    "        temp_flag = True\n",
    "        # print(action)\n",
    "        # print(functional_func(episode))\n",
    "        if functional_func(episode,epsilon):\n",
    "          data1_y_f_b.append(1)\n",
    "        else:\n",
    "          data1_y_f_b.append(0)\n",
    "        if reward_func(episode,threshold):\n",
    "          data1_y_b.append(1)\n",
    "        else:\n",
    "          data1_y_b.append(0)\n",
    "        # print(\"end\\n\\n\\n\")\n",
    "        # print(len(data1_y_b),\"len(input_episodes)\",len(input_episodes))\n",
    "        continue\n",
    "        # print(state[0])\n",
    "      ind = unique1.index(ab)\n",
    "      record[ind] = 1\n",
    "      # print(state, action)\n",
    "      assert len(data1_y_b)<len(input_episodes), \"assert\"\n",
    "      # if you want the frequency go with the next line \n",
    "      # record[ind] += 1\n",
    "    data1_x_b.append(record)\n",
    "\n",
    "  return data1_x_b, data1_y_b, data1_y_f_b\n",
    "\n",
    "\n",
    "def ML_first_representation(Abs_d,epsilon_functional_fault_boarder,model,ep,unique1):\n",
    "  d = Abs_d\n",
    "  # epsilon = 0.05\n",
    "  epsilon = epsilon_functional_fault_boarder\n",
    "  data1_x_b=[]\n",
    "  data1_y_b= [] \n",
    "  data1_y_f_b = []\n",
    "  functional_fault = False\n",
    "  reward_fault_threshold = 70\n",
    "  for episode in ep:\n",
    "    record = np.zeros(len(unique1))\n",
    "    for state, action in episode:\n",
    "      ab = abstract_state(model,state,d)\n",
    "      if ab == 'end':\n",
    "        # print(action)\n",
    "        if functional_fault:\n",
    "          data1_y_f_b.append(1)\n",
    "        else:\n",
    "          data1_y_f_b.append(0)\n",
    "        if action >= reward_fault_threshold:\n",
    "          data1_y_b.append(0)\n",
    "        else:\n",
    "          data1_y_b.append(1)\n",
    "        functional_fault=False\n",
    "        continue\n",
    "      if abs(state[0]) >= (2.4-epsilon) :\n",
    "        functional_fault = True\n",
    "      ind = unique1.index(ab)\n",
    "      # if len(w[0])>1:\n",
    "        # print('error len is greater than 1')\n",
    "      record[ind] = 1\n",
    "      # if you want the frequency go with the next line \n",
    "      # record[ind] += 1\n",
    "    data1_x_b.append(record)\n",
    "\n",
    "  return data1_x_b, data1_y_b, data1_y_f_b\n",
    "\n",
    "#report function to check the performance metrics of the model\n",
    "def report(model2,x_train, y_train,x_test, y_test):\n",
    "  print(\"********************** reporting the result of the model **************************\")\n",
    "  print('The score for train data is {0}'.format(model2.score(x_train,y_train)))\n",
    "  print('The score for test data is {0}'.format(model2.score(x_test,y_test)))\n",
    "\n",
    "\n",
    "  predictions_train = model2.predict(x_train)\n",
    "  predictions_test = model2.predict(x_test)\n",
    "\n",
    "  print(\"\\n\\n--------------------------------------recall---------------------------------\")\n",
    "\n",
    "  print('the test recall for the class yes is {0}'.format(metrics.recall_score(y_test,predictions_test, pos_label=1)))\n",
    "  print('the test recall for the class no is {0}'.format(metrics.recall_score(y_test,predictions_test, pos_label=0)))\n",
    "\n",
    "  print('the training recall for the class yes is {0}'.format(metrics.recall_score(y_train,predictions_train, pos_label=1)))\n",
    "  print('the training recall for the class no is {0}'.format(metrics.recall_score(y_train,predictions_train, pos_label=0)))\n",
    "\n",
    "\n",
    "  print(\"\\n\\n--------------------------------------precision------------------------------\")\n",
    "\n",
    "\n",
    "  print('the test precision for the class yes is {0}'.format(metrics.precision_score(y_test,predictions_test, pos_label=1)))\n",
    "  print('the test precision for the class no is {0}'.format(metrics.precision_score(y_test,predictions_test, pos_label=0)))\n",
    "\n",
    "  print('the training precision for the class yes is {0}'.format(metrics.precision_score(y_train,predictions_train, pos_label=1)))\n",
    "  print('the training precision for the class no is {0}'.format(metrics.precision_score(y_train,predictions_train, pos_label=0)))\n",
    "\n",
    "  print(\"\\n\\n\")\n",
    "  print(classification_report(y_test, predictions_test, target_names=['NO ','yes']))\n",
    "\n",
    "  tn, fp, fn, tp = confusion_matrix(y_test, predictions_test).ravel()\n",
    "  specificity = tn / (tn+fp)\n",
    "  print(\"\\n\\nspecifity :\",specificity)\n",
    "  print(\"\\n\\n--------------------------------------confusion----------------------------\")\n",
    "  CM = metrics.confusion_matrix(y_test, predictions_test)\n",
    "  print(\"The confusion Matrix:\")\n",
    "  print(CM)\n",
    "  print('the accuracy score in {0}\\n\\n'.format(accuracy_score(y_test, predictions_test)))\n",
    "  print(\"********************** plotting the confusion matrix & ROC curve **************************\")\n",
    "  plot_confusion_matrix(model2, x_test, y_test) #type: ignore\n",
    "  metrics.plot_roc_curve(model2, x_test, y_test)  #type: ignore\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def random_test_2(model, env, Num):\n",
    "  obs=env.reset()\n",
    "  counter = 1\n",
    "  episode_reward = 0.0\n",
    "  end =-1\n",
    "  lastpoint = -1\n",
    "  for i in range(Num):\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    episode_reward += reward\n",
    "    if done:\n",
    "      counter += 1\n",
    "      end = i\n",
    "      episode_reward = 0.0\n",
    "      obs = env.reset()\n",
    "  iter = deepcopy(counter)\n",
    "  u=1\n",
    "  while iter>1:\n",
    "    if env.info['mem'][-u][0]=='done': \n",
    "      lastpoint = -u\n",
    "      iter -= 1\n",
    "    u+=1\n",
    "  assert lastpoint != -1\n",
    "  assert end != -1\n",
    "  fin =Num - end\n",
    "  start = -Num -counter\n",
    "  randomtest = env.info['mem'][lastpoint:-fin] \n",
    "  ran_state = env.info['state'][(-counter+1):-1] \n",
    "  return randomtest , ran_state\n",
    "\n",
    "def fix_testing(testing_episodes,testing_states,Env2):\n",
    "  # TO DO: fix data typr for if condition change to nparray\n",
    "  buffer =[] \n",
    "  episodes_set = []\n",
    "  j=0\n",
    "  for i in range(len(testing_episodes)):\n",
    "    if testing_episodes[i][0] == 'done':\n",
    "      if i == 0:\n",
    "        continue\n",
    "      buffer.append(testing_episodes[i])\n",
    "      episodes_set.append(buffer)\n",
    "      buffer=[]\n",
    "    else:\n",
    "      buffer.append(testing_episodes[i])\n",
    "  if not (np.array(episodes_set[0][0][0])== np.array(Env2.set_state(testing_states[0]),dtype=\"float32\")).all():\n",
    "    del testing_states[0]\n",
    "  if not (np.array(episodes_set[0][0][0])==np.array(Env2.set_state(testing_states[0]),dtype=\"float32\")).all():\n",
    "    assert False, 'problem in starting states'\n",
    "  if len(episodes_set)!=len(testing_states):\n",
    "    del testing_states[-1]\n",
    "  if len(episodes_set)!=len(testing_states):\n",
    "    assert False, 'problem in data prepration'\n",
    "  return episodes_set , testing_states\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading a model without an environment, this model cannot be trained until it has a valid environment.\n"
     ]
    }
   ],
   "source": [
    "#Address of the trained RL model \n",
    "\n",
    "Dataset_path = \"Path to the dataset\"\n",
    "\n",
    "Drive_model  =f\"{Dataset_path}/Trained_agent/dqn-168.pkl\"\n",
    "\n",
    "\n",
    "env2 = gym.make('CartPole-v1')\n",
    "env2 = StoreWrapper(env2)\n",
    "model = DQN('MlpPolicy',env=env2, verbose=1)\n",
    "model = model.load(Drive_model)\n",
    "#########################################################  Read DATA and Load Model #############\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "def translator(episode,model, d, unique5):\n",
    "  \"\"\"\n",
    "  this function takes the concrete episodes and returns the encoded episodes \n",
    "  based on the presence and absence of the individuals  \n",
    "  :param 'episode': input episode\n",
    "  :param 'model': RL model\n",
    "  :param 'd': abstraction level = 1\n",
    "  :param 'unique5': abstract classes \n",
    "  :return: encoded episodse based on the presence and absence\n",
    "\n",
    "  \"\"\"\n",
    "  d=d\n",
    "  record = np.zeros(len(unique5))\n",
    "  for state, action in episode:\n",
    "    ab = abstract_state(model,state,d)\n",
    "    if ab == 'end':\n",
    "      continue\n",
    "    if ab in unique5:\n",
    "      ind = unique5.index(ab)\n",
    "      record[ind] = 1\n",
    "  return [record]\n",
    "\n",
    "def episode_player(episodes,d, abs_classes, model, monitor) -> list:\n",
    "  ''' This function replays the episodes and returns the risk of each step in each episode\n",
    "  :param 'episodes': input episodes\n",
    "  :param 'd': abstraction level \n",
    "  :param 'abs_classes': abstract classes\n",
    "  :param 'model': RL model\n",
    "  :param 'monitor': ML model\n",
    "  :return: risk of each step in each episode\n",
    "  \n",
    "  '''\n",
    "  episodes_risk=[]\n",
    "  for episode in episodes:\n",
    "    risk_array=[]\n",
    "    for step in range(len(episode)-1):\n",
    "      monitoring_data = translator(episode[:step],model,d,abs_classes)\n",
    "      Risk = monitor.predict_proba(monitoring_data)\n",
    "      risk_array.append(Risk[0][1])\n",
    "    episodes_risk.append(risk_array)\n",
    "  return episodes_risk\n",
    "\n",
    "def single_episode_player(episode,d, abs_classes, model, monitor) -> list:\n",
    "  ''' This function replays one episodes and returns the risk of each step in episode\n",
    "  :param 'episode': input episode\n",
    "  :param 'd': abstraction level\n",
    "  :param 'abs_classes': abstract classes\n",
    "  :param 'model': RL model\n",
    "  :param 'monitor': ML model\n",
    "  :return: risk of each step in episode\n",
    "  '''\n",
    "  risk_array=[]\n",
    "  for step in range(len(episode)-1):\n",
    "    monitoring_data = translator(episode[:step],model,d,abs_classes)\n",
    "    Risk = monitor.predict_proba(monitoring_data)\n",
    "    risk_array.append(Risk[0][1])\n",
    "  return risk_array\n",
    "\n",
    "\n",
    "def position_extractor(episode):\n",
    "    position =[]\n",
    "    for i in range(len(episode)-1):\n",
    "        position.append(episode[i][0][0])\n",
    "    return position\n",
    "    \n",
    "def velocity_extractor(episode):\n",
    "    velocity =[]\n",
    "    for i in range(len(episode)-1):\n",
    "        velocity.append(episode[i][0][1])\n",
    "    return velocity\n",
    "def angle_extractor(episode):\n",
    "    angle =[]\n",
    "    for i in range(len(episode)-1):\n",
    "        angle.append(episode[i][0][2])\n",
    "    return angle\n",
    "def angular_v_extractor(episode):\n",
    "    angular_v =[]\n",
    "    for i in range(len(episode)-1):\n",
    "        angular_v.append(episode[i][0][3])\n",
    "    return angular_v\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def Plot_all(data,abstraction_level,unique1,RL_model,Monitor,save=False,show=True,data_chunk=0,path='Plots_CartPole'):\n",
    "    '''plot risk and position snd velocity in one figure with 3 subplots\n",
    "    '''\n",
    "    ## TODO: write a function to return the setteings of the environment and the model\n",
    "\n",
    "    fig, axs = plt.subplots(5,figsize=(20, 18))\n",
    "    for i in range(len(data)):\n",
    "        axs[0].plot([i for i in range(len(data[i])-1)], single_episode_player(data[i],abstraction_level,unique1,RL_model,Monitor), label = f\"Episode {i}\")\n",
    "        axs[1].plot([i for i in range(len(data[i])-1)], position_extractor(data[i]), label = f\"Episode {i}\")\n",
    "        axs[2].plot([i for i in range(len(data[i])-1)], velocity_extractor(data[i]), label = f\"Episode {i}\")\n",
    "        axs[3].plot([i for i in range(len(data[i])-1)], angle_extractor(data[i]), label = f\"Episode {i}\")\n",
    "        axs[4].plot([i for i in range(len(data[i])-1)], angular_v_extractor(data[i]) ,label = f\"Episode {i}\")\n",
    "    axs[0].legend()\n",
    "    #plot title and labels\n",
    "    axs[0].set_title('Risk')\n",
    "    axs[1].legend()\n",
    "    axs[1].set_title('Position')\n",
    "    axs[2].set_title('Velocity')\n",
    "    axs[3].set_title('Angle')\n",
    "    axs[4].set_title('Angular Velocity')\n",
    "    axs[2].legend()\n",
    "    axs[3].legend()\n",
    "    axs[4].legend()\n",
    "    #set range of y axis\n",
    "    axs[0].set_ylim(-0.1, 1.1)\n",
    "    \n",
    "    current_time = datetime.now()\n",
    "    ID = current_time.strftime(\"%Y%m%d%H%M%S\")\n",
    "    if save:\n",
    "        fig.savefig(f'{path}/RPVAA2_C{data_chunk}_{ID}.png')\n",
    "    #close the plot\n",
    "    plt.close()\n",
    "\n",
    "def translate_episode_steps(episode,HD_model,translator,d,abs_classes):\n",
    "    translated_episode = []\n",
    "    for i in range(len(episode)-1):\n",
    "        translated_episode.append(translator(episode[:i],HD_model,d,abs_classes))\n",
    "    return translated_episode\n",
    "\n",
    "def translate_multiple_episodes_steps(episodes,HD_model,translator,d,abs_classes):\n",
    "    translated_episodes = []\n",
    "    for episode in episodes:\n",
    "        translated_buffer = []\n",
    "        for i in range(len(episode)-1):\n",
    "            translated_buffer.append(translator(episode[:i],HD_model,d,abs_classes))\n",
    "        translated_episodes.append(translated_buffer)\n",
    "    return translated_episodes\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def Forest_CI_multiple(translated_episodes,HD_model,chunk,abs_d,path = '/home'):\n",
    "    '''\n",
    "    size of translated_episodes is limited by the number of colors available for one plot\n",
    "    '''\n",
    "    plt.figure(figsize=(20, 12))\n",
    "    results_Arr=[]\n",
    "    r_arr=[]\n",
    "    E=0\n",
    "    colors = ['red','blue','green','yellow','black','purple','orange','pink','brown','grey','cyan','magenta','lime','olive','teal','navy','maroon','violet','turquoise','salmon','gold','coral','indigo','crimson','azure','beige','chocolate','lavender','plum','orchid','tan','khaki','wheat','silver','sienna','peru','peachpuff','papayawhip','mistyrose','moccasin','lemonchiffon','lawngreen','lightgreen','limegreen']\n",
    "    for translated_episode , plt_color in zip(translated_episodes,colors[:len(translated_episodes)]):\n",
    "        E+=1\n",
    "        num_time_steps = len(translated_episode) # Number of time steps\n",
    "        num_trees = HD_model.n_estimators\n",
    "        predictions = np.zeros((num_time_steps, num_trees))\n",
    "        for i, tree in enumerate(HD_model.estimators_):\n",
    "            for j in range(len(translated_episode)):\n",
    "                Risk = tree.predict_proba(translated_episode[j])[0][1]\n",
    "                predictions[j, i] = Risk\n",
    "        # Calculate the mean prediction for each time step\n",
    "        mean_predictions = np.mean(predictions, axis=1)\n",
    "\n",
    "        # Calculate the standard deviation for each time step\n",
    "        std_predictions = np.std(predictions, axis=1)\n",
    "\n",
    "        # Calculate the lower and upper bounds for the confidence intervals\n",
    "        confidence_level = 0.95 # Change as needed\n",
    "        z_score = scipy.stats.norm.ppf((1 + confidence_level) / 2)\n",
    "        lower_bounds = mean_predictions - z_score * std_predictions / np.sqrt(num_trees)\n",
    "        upper_bounds = mean_predictions + z_score * std_predictions / np.sqrt(num_trees)\n",
    "        difference = upper_bounds - lower_bounds\n",
    "        # Store the results in a dataframe\n",
    "        results = pd.DataFrame({\n",
    "            'Mean prediction': mean_predictions,\n",
    "            'Lower bound': lower_bounds,\n",
    "            'Upper bound': upper_bounds\n",
    "        })\n",
    "        # Save the results to a file\n",
    "        results_Arr.append(results)\n",
    "        r_arr.append([mean_predictions, lower_bounds, upper_bounds,difference])\n",
    "        plt.fill_between(range(num_time_steps), lower_bounds, upper_bounds, color=plt_color, alpha=0.2)\n",
    "        plt.plot(mean_predictions, color=plt_color, label=f'Episode {E}')\n",
    "        # set the y axis limits\n",
    "        plt.ylim(-0.1, 1.1)\n",
    "        # Add labels and title to the plot\n",
    "        plt.xlabel('Time step')\n",
    "        plt.ylabel('Prediction')\n",
    "        plt.title('Confidence Intervals of Random Forest Predictions')\n",
    "    # results_arr.append(r_arr)\n",
    "    # Save the plot as a file\n",
    "    current_time = datetime.now()\n",
    "    ID = current_time.strftime(\"%Y%m%d%H%M%S\")\n",
    "    plt.legend()\n",
    "    plt.savefig(f'{path}/confidence_intervals_{chunk}_{ID}.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "    pickle_path = f'CI/Abs_{abs_d}'\n",
    "    if not os.path.exists(pickle_path):\n",
    "        os.makedirs(pickle_path)\n",
    "    with open(f'{pickle_path}/results_arr_{chunk}.pkl', 'wb') as f:\n",
    "        pickle.dump(r_arr, f)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Confidence Intervals Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_set=[5,4,3,2,1.5,1,0.8,0.5,0.1,0.05,0.01,0.005]\n",
    "\n",
    "\n",
    "Drive_model  =f\"{Dataset_path}/Trained_agent/dqn-168.pkl\"\n",
    "\n",
    "\n",
    "env2 = gym.make('CartPole-v1')\n",
    "env2 = StoreWrapper(env2)\n",
    "model = DQN('MlpPolicy',env=env2, verbose=1)\n",
    "model = model.load(Drive_model)\n",
    "\n",
    "\n",
    "\n",
    "for d in d_set:\n",
    "    print(f'\\n\\n\\n#########################################\\nAbstraction level: {d}')\n",
    "    # unique1 = CartPole_config.unique1\n",
    "    #read unique1 from pickle\n",
    "    Read_from_data = True\n",
    "    if Read_from_data:\n",
    "        # assert False\n",
    "        with open(f'{Dataset_path}/ABS/Abstraction_data_final_{d}.pickle', 'rb') as file2:\n",
    "            unique1 = pickle.load(file2)\n",
    "        uni1=np.array(unique1)\n",
    "    else:\n",
    "        # print('Reading unique1 from file')\n",
    "        assert False , 'No abstraction data' # remove this line if you want to create abstract classes\n",
    "        unique1,uni1 = Abstract_classes(final_episodes,d,model)\n",
    "        with open(f'{Dataset_path}/ABS/Abstraction_data_final_CI_{d}.pickle', 'wb') as f:\n",
    "            pickle.dump(unique1, f)\n",
    "\n",
    "\n",
    "    ######################################################### Read FRT and FRTS from pickle #############\n",
    "    with open(f'{Dataset_path}/Random_episodes/FRT_test_68.pkl', 'rb') as file2:\n",
    "        FRT = pickle.load(file2)\n",
    "    \n",
    "    ######################################################### Read model from pickle #############\n",
    "    \n",
    "    HD_model_path1 = f'{Dataset_path}/ML_models'\n",
    "    HD_model_path = f'{HD_model_path1}/RF_FF_1rep_{d}.pickle'\n",
    "    with open(HD_model_path, 'rb') as file2:\n",
    "        RF_FF_1rep = pickle.load(file2)\n",
    "        \n",
    "    #########################################################  Plot Risk and Position #############\n",
    "    if not os.path.exists(\"CI\"):\n",
    "        os.makedirs(\"CI\")\n",
    "    newpath = 'CI/Abs_{d}'\n",
    "    if not os.path.exists(newpath):\n",
    "        os.makedirs(newpath)\n",
    "\n",
    "    Num_plot = 200 #number of episodes to plot\n",
    "    if len(FRT)<Num_plot:\n",
    "        print(\"number of available episodes is less than {Num_plot}}\")\n",
    "        Num_plot = len(FRT)\n",
    "    if d<0.05:\n",
    "        Num_plot = 100\n",
    "    print(\"Number of episodes to plot:\",Num_plot)\n",
    "    for i in tqdm(range(0,Num_plot,10), desc=\"Plotting\", total=Num_plot//10):\n",
    "        Forest_CI_multiple(translate_multiple_episodes_steps(FRT[i:i+10],model,translator,d,unique1),RF_FF_1rep,i,d,path=newpath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
